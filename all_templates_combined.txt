STATUS
commit 48cfff28e4ad198aa488439cc4468ae1ebc4d882 (HEAD -> main, origin/main, origin/HEAD)
Author: haroonn <76095593+Haroon-64@users.noreply.github.com>
Date:   Sun Jun 29 14:04:09 2025 +0530

    Update README.md

diff --git a/README.md b/README.md
index f5427fa..c8f481b 100644
--- a/README.md
+++ b/README.md
@@ -8,5 +8,5 @@
 - communicate with UI using FastAPI
   - receive config
   - send back generated code
-  - send training status, inference results
+  - send training command, inference results
 - run training or inference scripts and return result to UI



 

### ./models/layers.j2 ###   


{% macro linear(in_features, out_features, bias=True) %}
nn.Linear(
    in_features={{ in_features }},
    out_features={{ out_features }},
    bias={{ bias }}
)
{% endmacro %}

{% macro bilinear(in1_features, in2_features, out_features, bias=True) %}
nn.Bilinear(
    in1_features={{ in1_features }},
    in2_features={{ in2_features }},
    out_features={{ out_features }},
    bias={{ bias }}
)
{% endmacro %}

{% macro conv(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') %}
nn.Conv(
    in_channels={{ in_channels }},
    out_channels={{ out_channels }},
    kernel_size={{ kernel_size }},
    stride={{ stride }},
    padding={{ padding }},
    dilation={{ dilation }},
    groups={{ groups }},
    bias={{ bias }},
    padding_mode='{{ padding_mode }}'
)
{% endmacro %}

{% macro convtranspose(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') %}
nn.ConvTranspose(
    in_channels={{ in_channels }},
    out_channels={{ out_channels }},
    kernel_size={{ kernel_size }},
    stride={{ stride }},
    padding={{ padding }},
    output_padding={{ output_padding }},
    dilation={{ dilation }},
    groups={{ groups }},
    bias={{ bias }},
    padding_mode='{{ padding_mode }}'
)
{% endmacro %}

{% macro maxpool(kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False) %}
nn.MaxPool(
    kernel_size={{ kernel_size }},
    stride={{ stride if stride is not none else kernel_size }},
    padding={{ padding }},
    dilation={{ dilation }},
    ceil_mode={{ ceil_mode }}
)
{% endmacro %}

{% macro avgpool(kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, count_include_pad=True, divisor_override=None) %}
nn.AvgPool(
    kernel_size={{ kernel_size }},
    stride={{ stride if stride is not none else kernel_size }},
    padding={{ padding }},
    dilation={{ dilation }},
    ceil_mode={{ ceil_mode }},
    count_include_pad={{ count_include_pad }},
    divisor_override={{ divisor_override }}
)
{% endmacro %}

{% macro batchnorm(num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True) %}
nn.BatchNorm(
    num_features={{ num_features }},
    eps={{ eps }},
    momentum={{ momentum }},
    affine={{ affine }},
    track_running_stats={{ track_running_stats }}
)
{% endmacro %}

{% macro layernorm(normalized_shape, eps=1e-5, elementwise_affine=True) %}
nn.LayerNorm(
    normalized_shape={{ normalized_shape }},
    eps={{ eps }},
    elementwise_affine={{ elementwise_affine }}
)
{% endmacro %}

{% macro transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation='relu') %}
nn.Transformer(
    d_model={{ d_model }},
    nhead={{ nhead }},
    num_encoder_layers={{ num_encoder_layers }},
    num_decoder_layers={{ num_decoder_layers }},
    dim_feedforward={{ dim_feedforward }},
    dropout={{ dropout }},
    activation='{{ activation }}'
)
{% endmacro %}

{% macro multiheadattention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, kdim=None, vdim=None) %}
nn.MultiheadAttention(
    embed_dim={{ embed_dim }},
    num_heads={{ num_heads }},
    dropout={{ dropout }},
    bias={{ bias }},
    add_bias_kv={{ add_bias_kv }},
    kdim={{ kdim }},
    vdim={{ vdim }}
)
{% endmacro %}

{% macro dropout(p=0.5, inplace=False) %}
nn.Dropout(
    p={{ p }},
    inplace={{ inplace }}
)
{% endmacro %}

{% macro dropout_channel(p=0.5, inplace=False) %}
nn.DropoutChannel(
    p={{ p }},
    inplace={{ inplace }}
)
{% endmacro %}

{% macro embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, sparse=False) %}
nn.Embedding(
    num_embeddings={{ num_embeddings }},
    embedding_dim={{ embedding_dim }},
    padding_idx={{ padding_idx }},
    max_norm={{ max_norm }},
    sparse={{ sparse }}
)
{% endmacro %}

{% macro pixelshuffle(upscale_factor) %}
nn.PixelShuffle(
    upscale_factor={{ upscale_factor }}
)
{% endmacro %}

{% macro upsample(size=None, scale_factor=None, mode='nearest') %}
nn.Upsample(
    size={{ size }},
    scale_factor={{ scale_factor }},
    mode='{{ mode }}'
)
{% endmacro %}

{% macro lstm(input_size, hidden_size, num_layers=1, batch_first=False, bidirectional=False) %}
nn.LSTM(
    input_size={{ input_size }},
    hidden_size={{ hidden_size }},
    num_layers={{ num_layers }},
    batch_first={{ batch_first }},
    bidirectional={{ bidirectional }}
)
{% endmacro %}

{% macro flatten(start_dim=1, end_dim=-1) %}
nn.Flatten(
    start_dim={{ start_dim }},
    end_dim={{ end_dim }}
)
{% endmacro %}

{% macro unfold(kernel_size, stride=1, padding=0, dilation=1) %}
nn.Unfold(
    kernel_size={{ kernel_size }},
    stride={{ stride }},
    padding={{ padding }},
    dilation={{ dilation }}
)
{% endmacro %}


### ./models/audio.j2 ###


{% macro conformer(input_dim, num_heads, ffn_dim, num_layers, depthwise_conv_kernel_size, dropout=0.0, use_group_norm=False, convolution_first=False) %}
Conformer(
    input_dim={{ input_dim }},
    num_heads={{ num_heads }},
    ffn_dim={{ ffn_dim }},
    num_layers={{ num_layers }},
    depthwise_conv_kernel_size={{ depthwise_conv_kernel_size }},
    dropout={{ dropout }},
    use_group_norm={{ use_group_norm }},
    convolution_first={{ convolution_first }}
)
{% endmacro %}

{% macro wave2letter(num_classes=40, input_type='waveform', num_features=1) %}
Wave2Letter(
    num_classes={{ num_classes }},
    input_type='{{ input_type }}',
    num_features={{ num_features }}
)
{% endmacro %}

{% macro wavernn(upsample_scales, n_classes, hop_length, n_res_block=10, n_rnn=512, n_fc=512, kernel_size=5, n_freq=128, n_hidden=128, n_output=128) %}
WaveRNN(
    upsample_scales={{ upsample_scales }},
    n_classes={{ n_classes }},
    hop_length={{ hop_length }},
    n_res_block={{ n_res_block }},
    n_rnn={{ n_rnn }},
    n_fc={{ n_fc }},
    kernel_size={{ kernel_size }},
    n_freq={{ n_freq }},
    n_hidden={{ n_hidden }},
    n_output={{ n_output }}
)
{% endmacro %}

### ./models/ML.j2 ###




### ./models/text.j2 ###


{% macro glove(dim, name) %}
GloVe(
    dim={{ dim }},
    name='{{ name }}'
)
{% endmacro %}

{% macro fasttext(language) %}
FastText(
    language='{{ language }}'
)
{% endmacro %}

{% macro transformer(d_model, nhead, num_encoder_layers, num_decoder_layers) %}
Transformer(
    d_model={{ d_model }},
    nhead={{ nhead }},
    num_encoder_layers={{ num_encoder_layers }},
    num_decoder_layers={{ num_decoder_layers }}
)
{% endmacro %}

{% macro xlmroberta(num_classes, dropout, pooler_type) %}
XLMRoberta(
    num_classes={{ num_classes }},
    dropout={{ dropout }},
    pooler_type='{{ pooler_type }}'
)
{% endmacro %}

### ./models/image.j2 ###


{% macro resnet(pretrained, num_classes=1000, replace_stride_with_dilation=None) %}
ResNet(
    pretrained={{ pretrained }},
    num_classes={{ num_classes }},
    replace_stride_with_dilation={{ replace_stride_with_dilation }}
)
{% endmacro %}

{% macro efficientnet(width_mult, depth_mult, dropout) %}
EfficientNet(
    width_mult={{ width_mult }},
    depth_mult={{ depth_mult }},
    dropout={{ dropout }}
)
{% endmacro %}

{% macro vit(image_size, patch_size, num_layers, num_heads, hidden_dim) %}
VisionTransformer(
    image_size={{ image_size }},
    patch_size={{ patch_size }},
    num_layers={{ num_layers }},
    num_heads={{ num_heads }},
    hidden_dim={{ hidden_dim }}
)
{% endmacro %}

{% macro faster_rcnn(backbone, num_classes, min_size=None, max_size=None) %}
FasterRCNN(
    backbone={{ backbone }},
    num_classes={{ num_classes }},
    min_size={{ min_size }},
    max_size={{ max_size }}
)
{% endmacro %}

{% macro mask_rcnn(box_detections_per_img) %}
MaskRCNN(
    box_detections_per_img={{ box_detections_per_img }}
)
{% endmacro %}

{% macro deeplabv3(backbone, atrous_rates, num_classes) %}
DeepLabV3(
    backbone='{{ backbone }}',
    atrous_rates={{ atrous_rates }},
    num_classes={{ num_classes }}
)
{% endmacro %}


### ./data/transforms/audio.j2 ###


{% macro speed(orig_freq, factor) %}
Speed(
    orig_freq={{ orig_freq }},
    factor={{ factor }}
)
{% endmacro %}

{% macro amplitude_to_db(stype='"power"', top_db=None) %}
AmplitudeToDB(
    stype={{ stype }},
    top_db={{ top_db }}
)
{% endmacro %}

{% macro resample(orig_freq=16000, new_freq=16000, resampling_method='"sinc_interp_hann"', lowpass_filter_width=6, rolloff=0.99, beta=None, dtype=None) %}
Resample(
    orig_freq={{ orig_freq }},
    new_freq={{ new_freq }},
    resampling_method={{ resampling_method }},
    lowpass_filter_width={{ lowpass_filter_width }},
    rolloff={{ rolloff }},
    beta={{ beta }},
    dtype={{ dtype }}
)
{% endmacro %}

{% macro fade(fade_in_len=0, fade_out_len=0, fade_shape='"linear"') %}
Fade(
    fade_in_len={{ fade_in_len }},
    fade_out_len={{ fade_out_len }},
    fade_shape={{ fade_shape }}
)
{% endmacro %}

{% macro vol(gain, gain_type='"amplitude"') %}
Vol(
    gain={{ gain }},
    gain_type={{ gain_type }}
)
{% endmacro %}

{% macro loudness(sample_rate) %}
Loudness(
    sample_rate={{ sample_rate }}
)
{% endmacro %}

{% macro add_noise(waveform, noise, snr, lengths=None) %}
AddNoise(
    waveform={{ waveform }},
    noise={{ noise }},
    snr={{ snr }},
    lengths={{ lengths }}
)
{% endmacro %}

{% macro spectrogram(n_fft=400, win_length=None, hop_length=None, pad=0, window_fn='torch.hann_window', power=2, normalized=False, wkwargs=None, center=True, pad_mode='"reflect"', onesided=True, return_complex=None) %}
Spectrogram(
    n_fft={{ n_fft }},
    win_length={{ win_length }},
    hop_length={{ hop_length }},
    pad={{ pad }},
    window_fn={{ window_fn }},
    power={{ power }},
    normalized={{ normalized }},
    wkwargs={{ wkwargs }},
    center={{ center }},
    pad_mode={{ pad_mode }},
    onesided={{ onesided }},
    return_complex={{ return_complex }}
)
{% endmacro %}

{% macro inverse_spectrogram(n_fft=400, win_length=None, hop_length=None, pad=0, window_fn='torch.hann_window', normalized=False, wkwargs=None, center=True, pad_mode='"reflect"', onesided=True) %}
InverseSpectrogram(
    n_fft={{ n_fft }},
    win_length={{ win_length }},
    hop_length={{ hop_length }},
    pad={{ pad }},
    window_fn={{ window_fn }},
    normalized={{ normalized }},
    wkwargs={{ wkwargs }},
    center={{ center }},
    pad_mode={{ pad_mode }},
    onesided={{ onesided }}
)
{% endmacro %}

{% macro mel_scale(n_mels=128, sample_rate=16000, f_min=0.0, f_max=None, n_stft=201, norm=None, mel_scale='"htk"') %}
MelScale(
    n_mels={{ n_mels }},
    sample_rate={{ sample_rate }},
    f_min={{ f_min }},
    f_max={{ f_max }},
    n_stft={{ n_stft }},
    norm={{ norm }},
    mel_scale={{ mel_scale }}
)
{% endmacro %}

{% macro inverse_mel_scale(n_stft, n_mels=128, sample_rate=16000, f_min=0.0, f_max=None, norm=None, mel_scale='"htk"', driver='"gels"') %}
InverseMelScale(
    n_stft={{ n_stft }},
    n_mels={{ n_mels }},
    sample_rate={{ sample_rate }},
    f_min={{ f_min }},
    f_max={{ f_max }},
    norm={{ norm }},
    mel_scale={{ mel_scale }},
    driver={{ driver }}
)
{% endmacro %}

{% macro mel_spectrogram(sample_rate=16000, n_fft=400, win_length=None, hop_length=None, f_min=0.0, f_max=None, pad=0, n_mels=128, window_fn='torch.hann_window', power=2, normalized=False, wkwargs=None, center=True) %}
MelSpectrogram(
    sample_rate={{ sample_rate }},
    n_fft={{ n_fft }},
    win_length={{ win_length }},
    hop_length={{ hop_length }},
    f_min={{ f_min }},
    f_max={{ f_max }},
    pad={{ pad }},
    n_mels={{ n_mels }},
    window_fn={{ window_fn }},
    power={{ power }},
    normalized={{ normalized }},
    wkwargs={{ wkwargs }},
    center={{ center }}
)
{% endmacro %}

### ./data/transforms/ML.j2 ###




### ./data/transforms/text.j2 ###


{% macro RegexTokenizer(pattern) %}
RegexTokenizer(
    pattern={{ pattern }}
)
{% endmacro %}

{% macro SentencePieceTokenizer(sp_model) %}
SentencePieceTokenizer(
    sp_model={{ sp_model }}
)
{% endmacro %}

{% macro VocabTransform(vocab) %}
VocabTransform(
    vocab={{ vocab }}
)
{% endmacro %}

{% macro ToTensor(dtype) %}
ToTensor(
    dtype={{ dtype }}
)
{% endmacro %}

{% macro Truncate(max_seq_len) %}
Truncate(
    max_seq_len={{ max_seq_len }}
)
{% endmacro %}

{% macro PadTransform(max_length, pad_value=0) %}
PadTransform(
    max_length={{ max_length }},
    pad_value={{ pad_value }}
)
{% endmacro %}

{% macro AddToken(token, begin) %}
AddToken(
    token={{ token }},
    begin={{ begin }}
)
{% endmacro %}

{% macro BARTTokenizer(tokenizer) %}
BARTTokenizer(
    tokenizer={{ tokenizer }}
)
{% endmacro %}

{% macro LabelToIndex(label_names) %}
LabelToIndex(
    label_names={{ label_names }}
)
{% endmacro %}


### ./data/transforms/image.j2 ###


{# image_transforms.j2 #}
{% macro image_resize(size, interpolation="InterpolationMode.BILINEAR") -%}
T.Resize(size={{ size }}, interpolation={{ interpolation }})
{%- endmacro %}

{% macro image_random_crop(size, padding=None, pad_if_needed=False) -%}
T.RandomCrop(size={{ size }}{% if padding is not none %}, padding={{ padding }}{% endif %}, pad_if_needed={{ pad_if_needed|lower }})
{%- endmacro %}

{% macro image_random_horizontal_flip(p=0.5) -%}
T.RandomHorizontalFlip(p={{ p }})
{%- endmacro %}

{% macro image_random_rotation(degrees, interpolation="InterpolationMode.BICUBIC") -%}
T.RandomRotation(degrees={{ degrees }}, interpolation={{ interpolation }})
{%- endmacro %}

{% macro image_color_jitter(brightness=0, contrast=0, saturation=0, hue=0) -%}
T.ColorJitter(brightness={{ brightness }}, contrast={{ contrast }}, saturation={{ saturation }}, hue={{ hue }})
{%- endmacro %}

{% macro image_grayscale(num_output_channels=1) -%}
T.Grayscale(num_output_channels={{ num_output_channels }})
{%- endmacro %}

{% macro image_random_adjust_sharpness(sharpness_factor, p=0.5) -%}
T.RandomAdjustSharpness(sharpness_factor={{ sharpness_factor }}, p={{ p }})
{%- endmacro %}

{% macro image_to_tensor() -%}
T.ToTensor()
{%- endmacro %}

{% macro image_normalize(mean, std) -%}
T.Normalize(mean={{ mean }}, std={{ std }})
{%- endmacro %}

{% macro image_convert_dtype(dtype="torch.float32") -%}
T.ConvertImageDtype(dtype={{ dtype }})
{%- endmacro %}

{% macro image_random_erasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0) -%}
T.RandomErasing(p={{ p }}, scale={{ scale }}, ratio={{ ratio }}, value={{ value }})
{%- endmacro %}

{% macro image_gaussian_blur(kernel_size, sigma) -%}
T.GaussianBlur(kernel_size={{ kernel_size }}, sigma={{ sigma }})
{%- endmacro %}


### ./data/loaders/audio/classification.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
import torchaudio
from pathlib import Path

class AudioClassificationDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', duration={{ duration | default(5.0) }}, label_type='{{ label_type | default("folder-name") }}', label_map={{ label_map | default("None") }}, return_format='{{ return_format | default("dict") }}', multi_class={{ multi_class | default("False") }}, multi_label={{ multi_label | default("False") }}, split_type='{{ split_type | default("include") }}'):
        self.root = Path(root) / split
        self.duration = duration
        self.return_format = return_format
        self.label_type = label_type
        self.label_map = {{ label_map | default("self._infer_label_map()") }}
        self.sample_rate = 16000
        self.clip_samples = int(self.duration * self.sample_rate)
        self.data = self._load_files()

    def _infer_label_map(self):
        classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])
        return {cls: i for i, cls in enumerate(classes)}

    def _load_files(self):
        data = []
        for cls in self.label_map:
            for file in (self.root / cls).glob('*.wav'):
                data.append((file, self.label_map[cls]))
        return data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        path, label = self.data[idx]
        waveform, sr = torchaudio.load(path)
        waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)
        if waveform.shape[1] < self.clip_samples:
            waveform = torch.nn.functional.pad(waveform, (0, self.clip_samples - waveform.shape[1]))
        else:
            waveform = waveform[:, :self.clip_samples]

        {% if return_format == 'tuple' %}
        return waveform, label
        {% elif return_format == 'raw' %}
        return waveform
        {% else %}
        return {'audio': waveform, 'label': label}
        {% endif %}

def get_loader(root, batch_size=16, split='train', shuffle=True, **kwargs):
    dataset = AudioClassificationDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./data/loaders/audio/conversion.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
import torchaudio
from pathlib import Path

class AudioConversionDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', duration={{ duration | default(5.0) }}, return_format='{{ return_format | default("dict") }}'):
        self.root = Path(root) / split
        self.sample_rate = 16000
        self.clip_samples = int(duration * self.sample_rate)
        self.return_format = return_format
        self.data = list((self.root / 'input').glob('*.wav'))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        in_path = self.data[idx]
        out_path = Path(str(in_path).replace('input', 'target'))
        input_audio = torchaudio.load(in_path)[0][:, :self.clip_samples]
        target_audio = torchaudio.load(out_path)[0][:, :self.clip_samples]

        {% if return_format == 'tuple' %}
        return input_audio, target_audio
        {% elif return_format == 'raw' %}
        return input_audio
        {% else %}
        return {'input': input_audio, 'target': target_audio}
        {% endif %}

def get_loader(root, batch_size=4, split='train', shuffle=True, **kwargs):
    dataset = AudioConversionDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./data/loaders/audio/generation.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
import torchaudio
from pathlib import Path

class AudioGenerationDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', duration={{ duration | default(5.0) }}, include_prompt={{ include_prompt | default("True") }}, include_target={{ include_target | default("True") }}, audio_pair={{ audio_pair | default("False") }}, text_pair={{ text_pair | default("False") }}, return_format='{{ return_format | default("dict") }}'):
        self.root = Path(root) / split
        self.sample_rate = 16000
        self.clip_samples = int(duration * self.sample_rate)
        self.include_prompt = include_prompt
        self.include_target = include_target
        self.audio_pair = audio_pair
        self.text_pair = text_pair
        self.return_format = return_format
        self.data = self._load_data()

    def _load_data(self):
        {% if audio_pair %}
        return list((self.root / 'input').glob('*.wav'))
        {% elif text_pair %}
        return list((self.root / 'text').glob('*.txt'))
        {% else %}
        return list((self.root / 'prompt').glob('*.wav'))
        {% endif %}

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = {}
        {% if audio_pair %}
        in_path = self.data[idx]
        out_path = Path(str(in_path).replace('input', 'target'))
        item['prompt'] = torchaudio.load(in_path)[0][:, :self.clip_samples]
        item['target'] = torchaudio.load(out_path)[0][:, :self.clip_samples]
        {% elif text_pair %}
        txt_path = self.data[idx]
        audio_path = Path(str(txt_path).replace('text', 'target').replace('.txt', '.wav'))
        item['prompt'] = txt_path.read_text().strip()
        item['target'] = torchaudio.load(audio_path)[0][:, :self.clip_samples]
        {% else %}
        audio_path = self.data[idx]
        item['prompt'] = torchaudio.load(audio_path)[0][:, :self.clip_samples]
        {% endif %}

        {% if return_format == 'tuple' %}
        return tuple(item.values())
        {% elif return_format == 'raw' %}
        return item['prompt']
        {% else %}
        return item
        {% endif %}

def get_loader(root, batch_size=8, split='train', shuffle=True, **kwargs):
    dataset = AudioGenerationDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./data/loaders/audio/recognition.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
import torchaudio
from pathlib import Path

class AudioRecognitionDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', duration={{ duration | default(5.0) }}, return_format='{{ return_format | default("dict") }}'):
        self.root = Path(root) / split
        self.sample_rate = 16000
        self.clip_samples = int(duration * self.sample_rate)
        self.return_format = return_format
        self.audio_files = list((self.root / 'audio').glob('*.wav'))

    def __len__(self):
        return len(self.audio_files)

    def __getitem__(self, idx):
        audio_path = self.audio_files[idx]
        transcript_path = Path(str(audio_path).replace('audio', 'transcript').replace('.wav', '.txt'))
        waveform = torchaudio.load(audio_path)[0][:, :self.clip_samples]
        transcript = transcript_path.read_text().strip()

        {% if return_format == 'tuple' %}
        return waveform, transcript
        {% elif return_format == 'raw' %}
        return waveform
        {% else %}
        return {'audio': waveform, 'text': transcript}
        {% endif %}

def get_loader(root, batch_size=8, split='train', shuffle=True, **kwargs):
    dataset = AudioRecognitionDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./data/loaders/image/classification.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from pathlib import Path

class ImageClassificationDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', label_type='{{ label_type | default("folder-name") }}', label_map={{ label_map | default("None") }}, return_format='{{ return_format | default("dict") }}', split_type='{{ split_type | default("include") }}'):
        self.root = Path(root) / split
        self.label_map = {{ label_map | default("self._infer_label_map()") }}
        self.return_format = return_format
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor()
        ])
        self.data = self._load_data()

    def _infer_label_map(self):
        classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])
        return {cls: i for i, cls in enumerate(classes)}

    def _load_data(self):
        items = []
        for cls in self.label_map:
            for img_path in (self.root / cls).glob('*.jpg'):
                items.append((img_path, self.label_map[cls]))
        return items

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_path, label = self.data[idx]
        image = self.transform(Image.open(img_path).convert("RGB"))

        {% if return_format == 'tuple' %}
        return image, label
        {% elif return_format == 'raw' %}
        return image
        {% else %}
        return {'image': image, 'label': label}
        {% endif %}

def get_loader(root, batch_size=32, split='train', shuffle=True, **kwargs):
    dataset = ImageClassificationDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./data/loaders/image/generation.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from pathlib import Path

class ImageGenerationDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', return_format='{{ return_format | default("dict") }}', include_prompt={{ include_prompt | default("True") }}, include_target={{ include_target | default("True") }}, text_pair={{ text_pair | default("True") }}, split_type='{{ split_type | default("include") }}'):
        self.root = Path(root) / split
        self.include_prompt = include_prompt
        self.include_target = include_target
        self.text_pair = text_pair
        self.return_format = return_format
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor()
        ])
        self.data = self._load_data()

    def _load_data(self):
        if self.text_pair:
            prompt_files = sorted((self.root / 'prompt').glob('*.txt'))
            return [(p, Path(str(p).replace('prompt', 'target').replace('.txt', '.jpg'))) for p in prompt_files]
        else:
            return list((self.root / 'target').glob('*.jpg'))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        {% if text_pair %}
        prompt_path, image_path = self.data[idx]
        prompt = prompt_path.read_text().strip()
        image = self.transform(Image.open(image_path).convert("RGB"))
        {% else %}
        prompt = None
        image = self.transform(Image.open(self.data[idx]).convert("RGB"))
        {% endif %}

        {% if return_format == 'tuple' %}
        return (prompt, image) if prompt else (image,)
        {% elif return_format == 'raw' %}
        return image
        {% else %}
        out = {'image': image}
        {% if prompt %}
        out['prompt'] = prompt
        {% endif %}
        return out
        {% endif %}

def get_loader(root, batch_size=16, split='train', shuffle=True, **kwargs):
    dataset = ImageGenerationDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./data/loaders/image/object_detection.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import json
from pathlib import Path

class ObjectDetectionDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', return_format='{{ return_format | default("dict") }}', label_type='file', label_map={{ label_map | default("None") }}, split_type='{{ split_type | default("include") }}'):
        self.root = Path(root) / split
        self.return_format = return_format
        self.transform = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor()
        ])
        self.data = list((self.root / 'images').glob('*.jpg'))
        self.label_map = {{ label_map | default("self._infer_label_map()") }}

    def _infer_label_map(self):
        labels = set()
        for ann_path in (self.root / 'labels').glob('*.json'):
            anns = json.loads(ann_path.read_text())
            for obj in anns:
                labels.add(obj['label'])
        return {k: i for i, k in enumerate(sorted(labels))}

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_path = self.data[idx]
        ann_path = Path(str(img_path).replace('images', 'labels').replace('.jpg', '.json'))
        anns = json.loads(ann_path.read_text())

        boxes = []
        labels = []
        for ann in anns:
            boxes.append(ann['bbox'])  # [x_min, y_min, width, height]
            labels.append(self.label_map[ann['label']])
        boxes = torch.tensor(boxes, dtype=torch.float32)
        labels = torch.tensor(labels, dtype=torch.int64)

        image = self.transform(Image.open(img_path).convert("RGB"))

        {% if return_format == 'tuple' %}
        return image, {'boxes': boxes, 'labels': labels}
        {% elif return_format == 'raw' %}
        return image
        {% else %}
        return {'image': image, 'boxes': boxes, 'labels': labels}
        {% endif %}

def get_loader(root, batch_size=4, split='train', shuffle=True, **kwargs):
    dataset = ObjectDetectionDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=lambda x: tuple(zip(*x)))


### ./data/loaders/image/segmentation.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from pathlib import Path

class SegmentationDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', return_format='{{ return_format | default("dict") }}', label_map={{ label_map | default("None") }}, split_type='{{ split_type | default("include") }}'):
        self.root = Path(root) / split
        self.return_format = return_format
        self.image_paths = sorted((self.root / 'images').glob('*.jpg'))
        self.mask_paths = [Path(str(p).replace('images', 'masks').replace('.jpg', '.png')) for p in self.image_paths]
        self.img_transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor()
        ])
        self.mask_transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.PILToTensor()
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = self.img_transform(Image.open(self.image_paths[idx]).convert("RGB"))
        mask = self.mask_transform(Image.open(self.mask_paths[idx]))[0].long()

        {% if return_format == 'tuple' %}
        return image, mask
        {% elif return_format == 'raw' %}
        return image
        {% else %}
        return {'image': image, 'mask': mask}
        {% endif %}

def get_loader(root, batch_size=8, split='train', shuffle=True, **kwargs):
    dataset = SegmentationDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./data/loaders/text/classification.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from pathlib import Path
import json

class TextClassificationDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', tokenizer_name='{{ tokenizer_name | default("bert-base-uncased") }}', label_type='{{ label_type | default("folder-name") }}', label_map={{ label_map | default("None") }}, return_format='{{ return_format | default("dict") }}', max_length={{ max_length | default(512) }}, padding='{{ padding | default("max_length") }}', truncation='{{ truncation | default("right") }}', add_special_tokens={{ add_special_tokens | default("True") }}, return_attn_masks={{ return_attn_masks | default("True") }}, use_fast={{ use_fast | default("True") }}, multi_class={{ multi_class | default("False") }}, multi_label={{ multi_label | default("False") }}, split_type='{{ split_type | default("include") }}'):
        self.root = Path(root) / split
        self.label_type = label_type
        self.label_map = {{ label_map | default("self._infer_label_map()") }}
        self.return_format = return_format
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=use_fast)
        self.max_length = max_length
        self.padding = padding
        self.truncation = truncation == 'right'
        self.add_special_tokens = add_special_tokens
        self.return_attn_masks = return_attn_masks
        self.data = self._load_data()

    def _infer_label_map(self):
        if self.label_type == 'folder-name':
            classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])
            return {cls: i for i, cls in enumerate(classes)}
        raise NotImplementedError

    def _load_data(self):
        pairs = []
        for cls in self.label_map:
            for file in (self.root / cls).glob('*.txt'):
                text = file.read_text().strip()
                pairs.append((text, self.label_map[cls]))
        return pairs

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text, label = self.data[idx]
        enc = self.tokenizer(
            text,
            max_length=self.max_length,
            padding=self.padding,
            truncation=self.truncation,
            return_attention_mask=self.return_attn_masks,
            add_special_tokens=self.add_special_tokens,
            return_tensors='pt'
        )
        input_ids = enc['input_ids'].squeeze(0)
        attention_mask = enc['attention_mask'].squeeze(0) if self.return_attn_masks else None

        {% if return_format == 'tuple' %}
        return (input_ids, label)
        {% elif return_format == 'raw' %}
        return input_ids
        {% else %}
        out = {'input_ids': input_ids, 'label': label}
        {% if return_attn_masks %}
        out['attention_mask'] = attention_mask
        {% endif %}
        return out
        {% endif %}

def get_loader(root, batch_size=32, split='train', shuffle=True, **kwargs):
    dataset = TextClassificationDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./data/loaders/text/generation.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from pathlib import Path

class TextGenerationDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', tokenizer_name='{{ tokenizer_name | default("gpt2") }}', return_format='{{ return_format | default("dict") }}', max_length={{ max_length | default(512) }}, padding='{{ padding | default("max_length") }}', add_special_tokens={{ add_special_tokens | default("True") }}, return_attn_masks={{ return_attn_masks | default("True") }}, truncation='{{ truncation | default("right") }}', text_pair={{ text_pair | default("False") }}, include_prompt={{ include_prompt | default("True") }}, include_target={{ include_target | default("True") }}, use_fast={{ use_fast | default("True") }}):
        self.root = Path(root) / split
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=use_fast)
        self.max_length = max_length
        self.padding = padding
        self.return_format = return_format
        self.include_prompt = include_prompt
        self.include_target = include_target
        self.return_attn_masks = return_attn_masks
        self.text_pair = text_pair
        self.truncation = truncation == 'right'
        self.data = self._load_data()

    def _load_data(self):
        if self.text_pair:
            prompt_files = sorted((self.root / 'prompt').glob('*.txt'))
            return [(p.read_text().strip(), Path(str(p).replace('prompt', 'target')).read_text().strip()) for p in prompt_files]
        else:
            prompt_files = sorted((self.root / 'prompt').glob('*.txt'))
            return [(p.read_text().strip(), None) for p in prompt_files]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        prompt, target = self.data[idx]
        tokens = {}
        if self.text_pair and self.include_prompt and self.include_target:
            combined = prompt + self.tokenizer.sep_token + target
            tokens = self.tokenizer(combined, max_length=self.max_length, padding=self.padding, truncation=self.truncation, return_attention_mask=self.return_attn_masks, add_special_tokens=add_special_tokens, return_tensors='pt')
        elif self.include_prompt:
            tokens = self.tokenizer(prompt, max_length=self.max_length, padding=self.padding, truncation=self.truncation, return_attention_mask=self.return_attn_masks, add_special_tokens=add_special_tokens, return_tensors='pt')
        elif self.include_target and target:
            tokens = self.tokenizer(target, max_length=self.max_length, padding=self.padding, truncation=self.truncation, return_attention_mask=self.return_attn_masks, add_special_tokens=add_special_tokens, return_tensors='pt')

        input_ids = tokens['input_ids'].squeeze(0)
        attention_mask = tokens['attention_mask'].squeeze(0) if self.return_attn_masks else None

        {% if return_format == 'tuple' %}
        return input_ids, target
        {% elif return_format == 'raw' %}
        return input_ids
        {% else %}
        out = {'input_ids': input_ids}
        {% if return_attn_masks %}
        out['attention_mask'] = attention_mask
        {% endif %}
        {% if target %}
        out['target'] = target
        {% endif %}
        return out
        {% endif %}

def get_loader(root, batch_size=8, split='train', shuffle=True, **kwargs):
    dataset = TextGenerationDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./data/loaders/text/summarisation.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from pathlib import Path

class TextSummarizationDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', tokenizer_name='{{ tokenizer_name | default("bert-base-uncased") }}', return_format='{{ return_format | default("dict") }}', max_length={{ max_length | default(512) }}, padding='{{ padding | default("max_length") }}', add_special_tokens={{ add_special_tokens | default("True") }}, return_attn_masks={{ return_attn_masks | default("True") }}, truncation='{{ truncation | default("right") }}', use_fast={{ use_fast | default("True") }}):
        self.root = Path(root) / split
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=use_fast)
        self.max_length = max_length
        self.padding = padding
        self.return_format = return_format
        self.return_attn_masks = return_attn_masks
        self.truncation = truncation == 'right'
        self.data = self._load_data()

    def _load_data(self):
        srcs = sorted((self.root / 'document').glob('*.txt'))
        return [(doc.read_text().strip(), Path(str(doc).replace('document', 'summary')).read_text().strip()) for doc in srcs]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        document, summary = self.data[idx]
        enc = self.tokenizer(document, max_length=self.max_length, padding=self.padding, truncation=self.truncation, return_attention_mask=self.return_attn_masks, add_special_tokens=add_special_tokens, return_tensors='pt')
        input_ids = enc['input_ids'].squeeze(0)
        attention_mask = enc['attention_mask'].squeeze(0) if self.return_attn_masks else None

        {% if return_format == 'tuple' %}
        return input_ids, summary
        {% elif return_format == 'raw' %}
        return input_ids
        {% else %}
        out = {'input_ids': input_ids, 'summary': summary}
        {% if return_attn_masks %}
        out['attention_mask'] = attention_mask
        {% endif %}
        return out
        {% endif %}

def get_loader(root, batch_size=8, split='train', shuffle=True, **kwargs):
    dataset = TextSummarizationDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./data/loaders/text/translation.j2 ###


import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from pathlib import Path

class TextTranslationDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', tokenizer_name='{{ tokenizer_name | default("xlm-roberta-base") }}', return_format='{{ return_format | default("dict") }}', max_length={{ max_length | default(512) }}, padding='{{ padding | default("max_length") }}', add_special_tokens={{ add_special_tokens | default("True") }}, return_attn_masks={{ return_attn_masks | default("True") }}, truncation='{{ truncation | default("right") }}', use_fast={{ use_fast | default("True") }}):
        self.root = Path(root) / split
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=use_fast)
        self.max_length = max_length
        self.padding = padding
        self.return_format = return_format
        self.return_attn_masks = return_attn_masks
        self.truncation = truncation == 'right'
        self.data = self._load_data()

    def _load_data(self):
        srcs = sorted((self.root / 'source').glob('*.txt'))
        return [(s.read_text().strip(), Path(str(s).replace('source', 'target')).read_text().strip()) for s in srcs]
    
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        src, tgt = self.data[idx]
        enc = self.tokenizer(src, max_length=self.max_length, padding=self.padding, truncation=self.truncation, return_attention_mask=self.return_attn_masks, add_special_tokens=add_special_tokens, return_tensors='pt')
        input_ids = enc['input_ids'].squeeze(0)
        attention_mask = enc['attention_mask'].squeeze(0) if self.return_attn_masks else None

        {% if return_format == 'tuple' %}
        return input_ids, tgt
        {% elif return_format == 'raw' %}
        return input_ids
        {% else %}
        out = {'input_ids': input_ids, 'target': tgt}
        {% if return_attn_masks %}
        out['attention_mask'] = attention_mask
        {% endif %}
        return out
        {% endif %}

def get_loader(root, batch_size=8, split='train', shuffle=True, **kwargs):
    dataset = TextTranslationDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


### ./imports.j2 ###


import random
from pathlib import Path

import numpy as np
import torch

{% if config.data.data_format == "imagefolder" %}
from PIL import Image
from torchvision import transforms
{% endif %}

from torch.utils.data import DataLoader, Dataset

{# Model imports clearly #}
{% if config.model.use_pretrained %}
from torchvision import models
{% set nn_imported = False %}
{% else %}
import torch.nn as nn
{% set nn_imported = True %}
{% endif %}

{% if config.training.loss and not nn_imported %}
import torch.nn as nn
{% endif %}
import torch.nn as nn
{% endif %}

{% if config.training.metrics %}
import torchmetrics
{% endif %}

{# Additional conditional imports based on main_task and sub_task #}
{% if config.data.main_task %}
{% set main_task = config.data.main_task.lower() %}
{% if "audio" in main_task %}
import torchaudio
import torchaudio.transforms as audio_transforms
{% endif %}

{% if "text" in main_task %}
from torchtext.transforms import ToTensor
{% endif %}

{% if "text generation" == config.data.sub_task.lower() %}
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from torchmetrics.text import BLEUScore
{% endif %}

{% if "speech recognition" == config.data.sub_task.lower() %}
import torchaudio.functional as F
{% endif %}

{% if "image segmentation" == config.data.sub_task.lower() %}
import segmentation_models_pytorch as smp
{% endif %}

### ./setup.j2 ###


import torch
import random
import numpy as np

def set_seed(config, seed=None):
    seed = seed or config.training.get('seed', config.get('SEED', 42))
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def get_device(config, device=None):
    device_str = device or config.get('DEVICE', 'cpu')
    return torch.device(device_str if torch.cuda.is_available() else 'cpu')

### ./train/utils.j2 ###


{%- raw -%}
import torch.nn as nn
import torch.optim as optim

def get_criterion(config):
    loss_cfg = config.loss
    name = loss_cfg.name
    params = loss_cfg.get('params', {})
    return getattr(nn, name)(**params)

def get_optimizer(model, config):
    opt = config.optimizer
    Optim = getattr(optim, opt.name)
    params = opt.get('params', {})
    return Optim(model.parameters(), lr=config.training.learningRate, **params)
{%- endraw -%}

### ./train/eval_loop.j2 ###


{%- raw -%}
def evaluate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    with torch.no_grad():
        for batch in loader:
            inputs, targets = batch
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            total_loss += loss.item()
            correct += (outputs.argmax(dim=1) == targets).sum().item()
    return total_loss / len(loader), correct / len(loader.dataset)
{%- endraw -%}

### ./train/losses.j2 ###


# Loss function definition
{% set name = config.training.loss.name %}
{% set args = config.training.loss.params %}

{% if name == "cross_entropy" %}
criterion = torch.nn.CrossEntropyLoss(**{{ args }})
{% elif name == "bce" %}
criterion = torch.nn.BCELoss(**{{ args }})
{% elif name == "bce_with_logits" %}
criterion = torch.nn.BCEWithLogitsLoss(**{{ args }})
{% elif name == "mse" %}
criterion = torch.nn.MSELoss(**{{ args }})
{% elif name == "l1" %}
criterion = torch.nn.L1Loss(**{{ args }})
{% else %}
raise NotImplementedError("Unsupported loss: {{ name }}")
{% endif %}

### ./train/metrics.j2 ###


metrics = []
{% for m in config.training.metrics %}
{% if m.name == "accuracy" %}
metrics.append(torchmetrics.Accuracy(**{{ m.params }}))
{% elif m.name == "f1_score" %}
metrics.append(torchmetrics.F1Score(**{{ m.params }}))
{% elif m.name == "recall" %}
metrics.append(torchmetrics.Recall(**{{ m.params }}))
{% elif m.name == "mean_absolute_error" %}
metrics.append(torchmetrics.MeanAbsoluteError(**{{ m.params }}))
{% else %}
raise NotImplementedError("Unsupported metric: {{ m.name }}")
{% endif %}
{% endfor %}

### ./train/optimizers.j2 ###


# Optimizer definition
{% set name = config.training.optimizer.name %}
{% set args = config.training.optimizer.params %}

{% if name == "adam" %}
optimizer = torch.optim.Adam(model.parameters(), **{{ args }})
{% elif name == "sgd" %}
optimizer = torch.optim.SGD(model.parameters(), **{{ args }})
{% elif name == "rmsprop" %}
optimizer = torch.optim.RMSprop(model.parameters(), **{{ args }})
{% elif name == "adagrad" %}
optimizer = torch.optim.Adagrad(model.parameters(), **{{ args }})
{% elif name == "nadam" %}
optimizer = torch.optim.NAdam(model.parameters(), **{{ args }})
{% else %}
raise NotImplementedError("Unsupported optimizer: {{ name }}")
{% endif %}

### ./train/train_loop.j2 ###


def evaluate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    with torch.no_grad():
        for batch in loader:
            inputs, targets = batch["image"].to(device), batch["label"].to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            total_loss += loss.item()
            correct += (outputs.argmax(dim=1) == targets).sum().item()
    return total_loss / len(loader), correct / len(loader.dataset)

### ./train/monitoring.j2 ###


# Monitoring Setup

{% if config.monitoring.use_tensorboard %}
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()
{% endif %}

{% if config.monitoring.use_wandb %}
import wandb
wandb.init(project="{{ config.monitoring.project }}", config=config)
{% endif %}

{% if config.monitoring.use_mlflow %}
import mlflow
mlflow.start_run()
{% endif %}

{% if "threshold" in config.monitoring.threshold_alerts %}
# Add threshold-based alert logic here
print("Threshold alert not implemented")
{% endif %}

{% if "resource" in config.monitoring.resource_alerts %}
# Add resource monitoring code here
print("Resource alert not implemented")
{% endif %}
